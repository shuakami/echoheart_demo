{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notebook_intro_markdown"
   },
   "source": [
    "# 在 Google Colab 中进行 EchoHeart 的 Qwen 微调\n",
    "\n",
    "这个 Notebook 会自动配置环境、启动微调 (QLoRA)、运行测试、合并 LoRA 适配器并导出 GGUF 模型。\n",
    "\n",
    "**配置**：您可以通过修改下面的第一个代码单元格中的变量来指定要使用的基础模型和数据集。\n",
    "\n",
    "**步骤：**\n",
    "1. 配置变量、克隆/更新 GitHub 仓库并定义路径。\n",
    "2. 安装必要的依赖项。\n",
    "3. 运行训练脚本 (QLoRA)。\n",
    "4. (可选) 运行测试脚本与微调后的模型(适配器)交互。\n",
    "5. 合并 LoRA 适配器到基础模型。\n",
    "6. (可选) 将合并后的模型转换为 GGUF 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_and_setup_cell"
   },
   "outputs": [],
   "source": [
    "# 1. 配置、克隆/更新仓库并定义路径\n",
    "import os\n",
    "import sys\n",
    "# 尝试导入 rich 用于美化输出，如果失败则忽略\n",
    "try:\n",
    "    from rich import print\n",
    "    rich_available = True\n",
    "except ImportError:\n",
    "    rich_available = False\n",
    "    # 使用内置 print\n",
    "    pass\n",
    "\n",
    "# ==============================================================\n",
    "#                  主要配置区域 (在此处修改)\n",
    "# ==============================================================\n",
    "\n",
    "# --- A. ⚠️核心配置 ---\n",
    "# 指定要微调的基础模型 (Hugging Face 名称或路径)\n",
    "base_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\" # <--- 确认这里是正确的模型名称\n",
    "# 指定数据集文件路径 (相对于仓库根目录)\n",
    "dataset_file: str = \"data/converted_dataset.json\"\n",
    "\n",
    "# --- B. 训练超参数 (按需修改） ---\n",
    "num_train_epochs: int = 2          # 训练轮数\n",
    "learning_rate: float = 2e-4        # 学习率\n",
    "weight_decay: float = 0.01         # 权重衰减\n",
    "max_grad_norm: float = 1.0           # 梯度裁剪范数\n",
    "seed: int = 42                   # 随机种子\n",
    "\n",
    "# --- C. LoRA 特定参数 (按需修改) ---\n",
    "lora_r: int = 16                 # LoRA 秩\n",
    "lora_alpha: int = 32               # LoRA alpha\n",
    "lora_dropout: float = 0.05           # LoRA dropout\n",
    "\n",
    "# --- D. 保存与日志 (按需修改) ---\n",
    "save_steps: int = 25               # 每 N 步保存一次 checkpoint\n",
    "logging_steps: int = 5             # 每 N 步记录一次日志\n",
    "\n",
    "# --- E. 输出目录 (高级，通常自动生成) ---\n",
    "# 默认会根据 base_model_name 自动生成。\n",
    "# 如果要自定义，请将 None 替换为你的路径字符串(相对路径)，例如: \"output/my_custom_run\"\n",
    "custom_output_dir: str | None = None\n",
    "\n",
    "# --- F. ⚠️工作空间配置 (!!! 重要 !!!) ---\n",
    "# 指定仓库应该位于哪个父目录下。\n",
    "# 你要自己看你的环境是Linux 还是Windows 主要是取决于你的训练环境，比如说 /mnt/workspace, /workspace, /home/user 等。\n",
    "# 对于 Colab，固定为 /content。\n",
    "target_workspace_dir: str = \"/mnt/workspace\"  # <--- 修改这里来匹配你的环境!\n",
    "if 'google.colab' in sys.modules:\n",
    "    target_workspace_dir = \"/content\" # Colab 环境固定\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "#          仓库克隆/更新和路径动态确定 (通常无需修改)\n",
    "# ==============================================================\n",
    "\n",
    "# --- 仓库相关设置 ---\n",
    "repo_url: str = \"https://github.com/shuakami/echoheart_demo.git\"\n",
    "# 仓库目录名\n",
    "repo_dir_name: str = \"echoheart_demo\"\n",
    "\n",
    "# --- 计算准确的仓库路径 ---\n",
    "# 直接将工作空间路径和仓库目录名拼接\n",
    "repo_path: str = os.path.join(target_workspace_dir, repo_dir_name)\n",
    "repo_path = os.path.abspath(repo_path) # 确保是绝对路径\n",
    "\n",
    "if rich_available: print(f\"目标仓库路径: [bold magenta]{repo_path}[/bold magenta]\")\n",
    "else: print(f\"目标仓库路径: {repo_path}\")\n",
    "\n",
    "\n",
    "# --- 克隆或更新仓库 ---\n",
    "# 保存当前工作目录，以便后续恢复\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "# 确保目标工作空间目录存在\n",
    "os.makedirs(target_workspace_dir, exist_ok=True)\n",
    "\n",
    "# 切换到目标工作空间目录\n",
    "os.chdir(target_workspace_dir)\n",
    "if rich_available: print(f\"已切换到工作空间目录: [cyan]{os.getcwd()}[/cyan]\")\n",
    "else: print(f\"已切换到工作空间目录: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "if not os.path.exists(repo_path): # 检查的是拼接好的绝对路径\n",
    "  if rich_available: print(f\"\\n仓库不存在于 [cyan]{repo_path}[/cyan], 开始克隆...\")\n",
    "  else: print(f\"\\n仓库不存在于 {repo_path}, 开始克隆...\")\n",
    "  # 直接克隆到目标 repo_path (它现在是绝对路径)\n",
    "  clone_result = os.system(f\"git clone --depth 1 {repo_url} \\\"{repo_path}\\\"\")\n",
    "  if clone_result != 0:\n",
    "      # 克隆失败后切换回原始目录\n",
    "      os.chdir(original_cwd)\n",
    "      raise RuntimeError(f\"Git 克隆失败，请检查仓库 URL 和目标路径: {repo_path}\")\n",
    "  os.chdir(repo_path) # 克隆成功后进入仓库目录\n",
    "else:\n",
    "  if rich_available: print(f\"\\n仓库已存在于 [cyan]{repo_path}[/cyan].\")\n",
    "  else: print(f\"\\n仓库已存在于 {repo_path}.\")\n",
    "  os.chdir(repo_path) # 如果已存在，也进入仓库目录\n",
    "  if rich_available: print(\"\\n尝试拉取最新更改...\")\n",
    "  else: print(\"\\n尝试拉取最新更改...\")\n",
    "  # 使用 --no-edit 避免潜在的编辑器冲突，并确保拉取成功\n",
    "  pull_result = os.system(\"git pull origin master --no-edit --ff-only\")\n",
    "  if pull_result != 0:\n",
    "      if rich_available: print(f\"[yellow]Git pull 失败 (可能由于本地更改或非快进合并). 将继续使用本地版本.[/yellow]\")\n",
    "      else: print(f\"警告: Git pull 失败。将继续使用本地版本。\")\n",
    "\n",
    "# --- 确认最终的仓库路径 ---\n",
    "final_repo_path = os.getcwd()\n",
    "if final_repo_path != repo_path:\n",
    "     print(f\"[bold red]警告：最终工作目录 ({final_repo_path}) 与预期仓库路径 ({repo_path}) 不符！请检查逻辑。[/bold red]\")\n",
    "     repo_path = final_repo_path\n",
    "\n",
    "if rich_available: print(f\"\\n[green]确认当前工作目录 (仓库路径):[/green] [bold magenta]{repo_path}[/bold magenta]\")\n",
    "else: print(f\"\\n确认当前工作目录 (仓库路径): {repo_path}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "#          基于动态路径的计算 (通常无需修改)\n",
    "# ==============================================================\n",
    "\n",
    "# --- 确定最终适配器输出目录 ---\n",
    "# 如果用户没有设置 custom_output_dir，则自动生成\n",
    "if custom_output_dir:\n",
    "    # 用户自定义的是相对路径\n",
    "    adapter_output_rel_dir: str = custom_output_dir\n",
    "    if rich_available: print(f\"使用自定义适配器相对输出目录: [blue]{adapter_output_rel_dir}[/blue]\")\n",
    "    else: print(f\"使用自定义适配器相对输出目录: {adapter_output_rel_dir}\")\n",
    "else:\n",
    "    # 自动生成相对路径\n",
    "    adapter_output_rel_dir: str = f\"output/{base_model_name.split('/')[-1]}-qlora-ft\"\n",
    "    if rich_available: print(f\"使用自动生成的适配器相对输出目录: [blue]{adapter_output_rel_dir}[/blue]\")\n",
    "    else: print(f\"使用自动生成的适配器相对输出目录: {adapter_output_rel_dir}\")\n",
    "\n",
    "# --- 计算绝对路径 (!!! 基于正确的 repo_path !!!) ---\n",
    "adapter_path: str = os.path.join(repo_path, adapter_output_rel_dir)\n",
    "dataset_abs_path: str = os.path.join(repo_path, dataset_file)\n",
    "\n",
    "# --- 合并模型和 GGUF 的输出路径 (保持相对路径结构) ---\n",
    "# 合并模型的目录名，基于适配器目录名\n",
    "merged_model_output_rel_dir: str = adapter_output_rel_dir.replace('-qlora-ft', '-merged-ft')\n",
    "# 合并模型的绝对路径\n",
    "merged_model_path: str = os.path.join(repo_path, merged_model_output_rel_dir)\n",
    "# GGUF 文件名\n",
    "gguf_output_filename: str = f\"gguf-model-f16.gguf\"\n",
    "# GGUF 的绝对输出路径 (放在合并后的模型目录里)\n",
    "gguf_output_abs_path: str = os.path.join(merged_model_path, gguf_output_filename)\n",
    "\n",
    "# ==============================================================\n",
    "#          路径验证\n",
    "# ==============================================================\n",
    "print(\"\\n--- 路径验证 ---\")\n",
    "print(f\"仓库根目录 (repo_path): {repo_path}\")\n",
    "print(f\"适配器目录 (adapter_path): {adapter_path}\")\n",
    "print(f\"数据集文件 (dataset_abs_path): {dataset_abs_path}\")\n",
    "print(f\"合并模型目录 (merged_model_path): {merged_model_path}\")\n",
    "print(f\"GGUF 输出路径 (gguf_output_abs_path): {gguf_output_abs_path}\")\n",
    "print(\"------------------\")\n",
    "\n",
    "# ==============================================================\n",
    "#          最终配置打印与检查\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n--- 最终配置信息 ---\")\n",
    "print(f\"基础模型: {base_model_name}\")\n",
    "print(f\"数据集文件 (绝对路径): {dataset_abs_path}\")\n",
    "print(f\"适配器输出目录 (绝对路径): {adapter_path}\")\n",
    "print(f\"学习率: {learning_rate}, Epochs: {num_train_epochs}, LoRA r: {lora_r}\")\n",
    "print(f\"合并模型输出目录 (绝对路径): {merged_model_path}\")\n",
    "print(f\"GGUF 输出文件 (绝对路径): {gguf_output_abs_path}\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "# 确保数据集文件存在\n",
    "if not os.path.exists(dataset_abs_path):\n",
    "  err_msg = f\"错误：指定的数据集文件不存在: {dataset_abs_path}\"\n",
    "  if rich_available: print(f\"\\n[bold red]{err_msg}[/bold red]\")\n",
    "  else: print(f\"\\n{err_msg}\")\n",
    "  # 引发错误停止执行\n",
    "  raise FileNotFoundError(err_msg)\n",
    "else:\n",
    "  ok_msg = f\"数据集文件确认存在: {dataset_abs_path}\"\n",
    "  if rich_available: print(f\"\\n[green]{ok_msg}[/green]\")\n",
    "  else: print(f\"\\n{ok_msg}\")\n",
    "\n",
    "print(\"\\n最终工作目录 (应为仓库根目录):\")\n",
    "# 使用 os.system 执行 pwd/cd，确保显示的是 shell 的当前目录\n",
    "if os.name == 'nt': # Windows系统\n",
    "    os.system(\"cd\")\n",
    "else: # Linux/macOS系统\n",
    "    os.system(\"pwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps_cell"
   },
   "outputs": [],
   "source": [
    "# 2. 安装依赖\n",
    "# 安装 ModelScope (如果需要)\n",
    "try:\n",
    "    # 检查是否可以连接到 Hugging Face\n",
    "    import socket\n",
    "    can_connect_to_hf = False\n",
    "    try:\n",
    "        socket.setdefaulttimeout(2.0)\n",
    "        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((\"huggingface.co\", 443))\n",
    "        can_connect_to_hf = True\n",
    "        print(\"已检测到 Hugging Face 连接正常，不需要使用备选下载源。\")\n",
    "    except:\n",
    "        print(\"无法连接到 Hugging Face，将尝试设置 ModelScope 作为备选下载源。\")\n",
    "        \n",
    "    if not can_connect_to_hf:\n",
    "        print(\"正在安装 ModelScope 以便从国内镜像下载模型...\")\n",
    "        # 安装 ModelScope\n",
    "        !pip install -q modelscope\n",
    "        \n",
    "        # 设置 ModelScope 环境变量\n",
    "        import os\n",
    "        os.environ[\"MODELSCOPE_CACHE\"] = os.path.expanduser(\"~/.cache/modelscope/hub\")\n",
    "        os.environ[\"VLLM_USE_MODELSCOPE\"] = \"True\"\n",
    "        os.environ[\"USE_MODELSCOPE_CACHE\"] = \"1\"\n",
    "        \n",
    "        print(f\"ModelScope 已设置为模型下载源，模型将缓存在: {os.environ['MODELSCOPE_CACHE']}\")\n",
    "        \n",
    "        # 如果需要登录？\n",
    "        # from modelscope import HubApi\n",
    "        # api = HubApi()\n",
    "        # api.login(\"YOUR_MODELSCOPE_TOKEN\") # 替换为您的 token\n",
    "        # print(\"已登录 ModelScope\")\n",
    "except Exception as e:\n",
    "    print(f\"ModelScope 设置失败: {e}\")\n",
    "    print(\"将继续使用 Hugging Face 下载模型。\")\n",
    "\n",
    "# 安装依赖\n",
    "print(\"Installing dependencies from requirements.txt...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"Applying dependency fixes...\")\n",
    "!pip install fsspec==2024.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCd-DEGuXUJ6"
   },
   "outputs": [],
   "source": [
    "# 3. 运行训练脚本 (QLoRA)\n",
    "print(\"Starting QLoRA training...\")\n",
    "\n",
    "train_command = (\n",
    "    f\"python train.py \"\n",
    "    f\"--base_model_name \\\"{base_model_name}\\\" \"\n",
    "    f\"--dataset_file \\\"{dataset_file}\\\" \"\n",
    "    f\"--output_dir \\\"{adapter_path}\\\" \" \n",
    "    f\"--num_train_epochs {num_train_epochs} \"\n",
    "    f\"--learning_rate {learning_rate} \"\n",
    "    f\"--weight_decay {weight_decay} \"\n",
    "    f\"--max_grad_norm {max_grad_norm} \"\n",
    "    f\"--lora_r {lora_r} \"\n",
    "    f\"--lora_alpha {lora_alpha} \"\n",
    "    f\"--lora_dropout {lora_dropout} \"\n",
    "    f\"--save_steps {save_steps} \"\n",
    "    f\"--logging_steps {logging_steps} \"\n",
    "    f\"--seed {seed}\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Running Training Command ---\")\n",
    "print(train_command)\n",
    "print(\"------------------------------\\n\")\n",
    "\n",
    "# 执行命令\n",
    "!{train_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GFKaIkLXUJ6"
   },
   "source": [
    "## 训练完成！\n",
    "\n",
    "微调后的模型保存在 Colab 环境文件系统的 `output/qwen-ft` 目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "added_test_step"
   },
   "outputs": [],
   "source": [
    "# 4. 运行测试脚本\n",
    "print(\"Starting non-interactive testing session...\")\n",
    "!python test_model.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gguf_markdown"
   },
   "source": [
    "## 5. (可选) 转换为 GGUF 格式\n",
    "\n",
    "转换后的文件将尝试保存在 `{gguf_output_file}`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "added_gguf_step"
   },
   "outputs": [],
   "source": [
    "# 运行 LoRA 合并脚本\n",
    "print(\"Starting LoRA merge...\")\n",
    "!python merge_lora.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\" --output_path \"{merged_model_path}\"\n",
    "\n",
    "# 转换为 GGUF\n",
    "print(\"Starting GGUF conversion...\")\n",
    "!python convert_to_gguf.py --model_dir \"{merged_model_path}\" --output_file \"{gguf_output_abs_path}\" --out_type f16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (可选) 查看 train.py 的高级参数\n",
    "\n",
    "运行下面的代码单元格可以显示 `train.py` 脚本支持的所有命令行参数及其说明和默认值。\n",
    "如果您想覆盖默认设置（例如调整学习率、LoRA rank、保存步数等），可以在第 3 步运行训练时手动添加这些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --help"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
